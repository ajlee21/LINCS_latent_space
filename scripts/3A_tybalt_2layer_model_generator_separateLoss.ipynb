{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.103018Z",
     "start_time": "2019-02-13T23:00:18.130817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee\n",
    "# (created Janurary 2019) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with 2-hidden layers\n",
    "# \n",
    "# Note: Need to use python 3 to support '*' syntax change\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from functions.my_classes import DataGenerator\n",
    "#from functions.vae_utils import VariationalLayer, WarmUpCallback, LossCallback\n",
    "from functions.models import Tybalt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.125446Z",
     "start_time": "2019-02-13T23:00:19.104636Z"
    }
   },
   "outputs": [],
   "source": [
    "# To ensure reproducibility using Keras during development\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "randomState = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.177295Z",
     "start_time": "2019-02-13T23:00:19.126657Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1317819\n",
      "1319\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "learning_rate_model = 0.00001\n",
    "epochs_model = 5\n",
    "kappa_model = 0.01\n",
    "\n",
    "intermediate_dim_model = 100\n",
    "latent_dim_model = 10\n",
    "epsilon_std_model = 1.0\n",
    "beta_model = K.variable(0)\n",
    "\n",
    "chunk_size = 100\n",
    "\n",
    "# Get dimensions of datasets\n",
    "train_dim_file =  os.path.join(os.path.dirname(os.getcwd()), \"metadata\", \"train_dim.pickle\")\n",
    "val_dim_file =  os.path.join(os.path.dirname(os.getcwd()), \"metadata\", \"validation_dim.pickle\")\n",
    "\n",
    "with open(train_dim_file, 'rb') as f:\n",
    "    num_train_samples, num_genes = pickle.load(f)\n",
    "with open(val_dim_file, 'rb') as f:\n",
    "    num_val_samples, num_genes = pickle.load(f)\n",
    "\n",
    "original_dim_model = num_genes\n",
    "print(num_train_samples)#num_samples_train = 1319122\n",
    "print(num_val_samples)#num_samples_val = 1319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.217193Z",
     "start_time": "2019-02-13T23:00:19.179846Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load gene expression data using generator \n",
    "train_file =  \"/home/alexandra/Documents/Data/LINCS_tuning/train_model_input.txt.xz\"\n",
    "validation_file = \"/home/alexandra/Documents/Data/LINCS_tuning/validation_model_input.txt.xz\"\n",
    "\n",
    "training_generator = DataGenerator(train_file, chunk_size, num_train_samples)\n",
    "validation_generator = DataGenerator(validation_file, chunk_size, num_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.233555Z",
     "start_time": "2019-02-13T23:00:19.218625Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model \n",
    "model = Tybalt(original_dim=original_dim_model,\n",
    "                 hidden_dim=intermediate_dim_model,\n",
    "                 latent_dim=latent_dim_model,\n",
    "                 batch_size=chunk_size,\n",
    "                 epochs=epochs_model,\n",
    "                 learning_rate=learning_rate_model,\n",
    "                 kappa=kappa_model,\n",
    "                 beta=beta_model,\n",
    "                 epsilon_std=epsilon_std_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.252929Z",
     "start_time": "2019-02-13T23:00:19.234766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output files\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}latent_stats.tsv\".format(latent_dim_model))\n",
    "hist_plot_file = os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}latent_hist.png\".format(latent_dim_model))\n",
    "\n",
    "encoded_file = os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"train_input_2layer_{}latent_encoded.txt\".format(latent_dim_model))\n",
    "\n",
    "model_encoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_encoder_model.h5\".format(latent_dim_model))\n",
    "weights_encoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_encoder_weights.h5\".format(latent_dim_model))\n",
    "model_decoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_decoder_model.h5\".format(latent_dim_model))\n",
    "weights_decoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_decoder_weights.h5\".format(latent_dim_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.582827Z",
     "start_time": "2019-02-13T23:00:19.254260Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 978)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          97900       input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 100)          400         dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 100)          0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           1010        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 10)           1010        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 10)           40          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10)           40          dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 10)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 10)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 10)           0           activation_2[0][0]               \n",
      "                                                                 activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 978)          99878       lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "custom_variational_layer_1 (Cus [(None, 978), (None, 0           input_1[0][0]                    \n",
      "                                                                 sequential_1[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 200,278\n",
      "Trainable params: 200,038\n",
      "Non-trainable params: 240\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/Documents/Repos/LINCS_latent_space/scripts/functions/models.py:85: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n",
      "  self.vae.compile(optimizer=adam, loss=None, loss_weights=[self.beta])\n"
     ]
    }
   ],
   "source": [
    "# Compile Model\n",
    "model.build_encoder_layer()\n",
    "model.build_decoder_layer()\n",
    "model.compile_vae()\n",
    "model.get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.678603Z",
     "start_time": "2019-02-13T23:00:19.586420Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "model_architecture_file = os.path.join(os.path.dirname(os.getcwd()),'stats', 'vae_architecture.png')\n",
    "model.visualize_architecture(model_architecture_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.806376Z",
     "start_time": "2019-02-13T23:00:19.680120Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating callback loss...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Tybalt' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documents/Repos/LINCS_latent_space/scripts/functions/models.py\u001b[0m in \u001b[0;36mtrain_vae\u001b[0;34m(self, train_df, test_df, separate_loss)\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"updating callback loss...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             tybalt_loss_cbk = LossCallback(training_data=train_df,\n\u001b[0;32m--> 119\u001b[0;31m                                            \u001b[0mencoder_cbk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                                            \u001b[0mdecoder_cbk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                                            original_dim=self.original_dim)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tybalt' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.train_vae(train_df=training_generator,\n",
    "                test_df=validation_generator,\n",
    "               separate_loss=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.830542Z",
     "start_time": "2019-02-13T23:00:19.807440Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tybalt' object has no attribute 'hist'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-a82100946285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_training_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stats'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training.pdf'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvisualize_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_training_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Repos/LINCS_latent_space/scripts/functions/models.py\u001b[0m in \u001b[0;36mvisualize_training\u001b[0;34m(self, output_file)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvisualize_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# Visualize training performance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mhistory_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tybalt' object has no attribute 'hist'"
     ]
    }
   ],
   "source": [
    "model_training_file = os.path.join(os.path.dirname(os.getcwd()), 'stats', 'training.pdf')\n",
    "model.visualize_training(model_training_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.831307Z",
     "start_time": "2019-02-13T23:00:18.147Z"
    }
   },
   "outputs": [],
   "source": [
    "model_compression = model.compress(rnaseq_df)\n",
    "model_file = os.path.join(os.path.dirname(os.getcwd()), 'data', 'encoded_rnaseq_twohidden_100model.tsv.gz')\n",
    "model_compression.to_csv(model_file, sep='\\t', compression='gzip')\n",
    "model_compression.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.831904Z",
     "start_time": "2019-02-13T23:00:18.148Z"
    }
   },
   "outputs": [],
   "source": [
    "model_weights = model.get_decoder_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-13T23:00:19.832562Z",
     "start_time": "2019-02-13T23:00:18.150Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model_file = os.path.join(os.path.dirname(os.getcwd()),'models', 'encoder_twohidden100_vae.hdf5')\n",
    "decoder_model_file = os.path.join(os.path.dirname(os.getcwd()),'models', 'decoder_twohidden100_vae.hdf5')\n",
    "model.save_models(encoder_model_file, decoder_model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pa]",
   "language": "python",
   "name": "conda-env-Pa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
