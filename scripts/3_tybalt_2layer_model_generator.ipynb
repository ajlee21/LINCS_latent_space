{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:11:19.171859Z",
     "start_time": "2019-02-07T22:11:18.258591Z"
    }
   },
   "source": [
    "# Train 2-layer Tybalt\n",
    "\n",
    "**By Alexandra Lee**\n",
    "\n",
    "**created December 2018**\n",
    "\n",
    "Encode Pseudomonas gene expression data into low dimensional latent space using Tybalt with 2-hidden layers\n",
    " \n",
    "Note: Need to use python 3 to support '*' syntax change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from functions.my_classes import DataGenerator\n",
    "from functions.helper_ae import sampling_maker, CustomVariationalLayer, WarmUpCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:11:19.193522Z",
     "start_time": "2019-02-07T22:11:19.173517Z"
    }
   },
   "outputs": [],
   "source": [
    "# To ensure reproducibility using Keras during development\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "randomState = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize hyper parameters\n",
    "\n",
    "1.  learning rate: \n",
    "2.  batch size: Total number of training examples present in a single batch.  Iterations is the number of batches needed to complete one epoch\n",
    "3.  epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "4.  kappa: warmup\n",
    "5.  original dim: dimensions of the raw data\n",
    "6.  latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "    Note: intrinsic latent space dimension unknown\n",
    "7.  epsilon std: \n",
    "8.  beta: Threshold value for ReLU?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:11:19.235473Z",
     "start_time": "2019-02-07T22:11:19.195497Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10553\n",
      "2638\n"
     ]
    }
   ],
   "source": [
    "# Initialize parameters\n",
    "learning_rate = 0.00001\n",
    "epochs = 100\n",
    "kappa = 0.01\n",
    "\n",
    "intermediate_dim = 200\n",
    "latent_dim = 100\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "chunk_size = 1000\n",
    "\n",
    "subsample_dataset = \"subsample_13K_validation_0.2\"\n",
    "\n",
    "# Get dimensions of datasets\n",
    "train_dim_file =  os.path.join(os.path.dirname(os.getcwd()), \"metadata\", subsample_dataset, \"train_tune_dim.pickle\")\n",
    "val_dim_file =  os.path.join(os.path.dirname(os.getcwd()), \"metadata\", subsample_dataset, \"validation_tune_dim.pickle\")\n",
    "\n",
    "with open(train_dim_file, 'rb') as f:\n",
    "    num_train_samples, num_genes = pickle.load(f)\n",
    "with open(val_dim_file, 'rb') as f:\n",
    "    num_val_samples, num_genes = pickle.load(f)\n",
    "\n",
    "original_dim_model = num_genes\n",
    "print(num_train_samples)#num_samples_train = 1319122\n",
    "print(num_val_samples)#num_samples_val = 1319"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:11:19.275833Z",
     "start_time": "2019-02-07T22:11:19.237663Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load gene expression data using generator \n",
    "train_file =  \"/home/alexandra/Documents/Data/LINCS_tuning/\"+subsample_dataset+\"/train_model_input.txt.xz\"\n",
    "validation_file = \"/home/alexandra/Documents/Data/LINCS_tuning/\"+subsample_dataset+\"/validation_model_input.txt.xz\"\n",
    "\n",
    "training_generator = DataGenerator(train_file, chunk_size, num_train_samples)\n",
    "validation_generator = DataGenerator(validation_file, chunk_size, num_val_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:11:19.292248Z",
     "start_time": "2019-02-07T22:11:19.277144Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output files\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}latent_stats.tsv\".format(latent_dim))\n",
    "hist_plot_file = os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}latent_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file = os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"train_input_2layer_{}latent_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_encoder_model.h5\".format(latent_dim))\n",
    "weights_encoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_decoder_model.h5\".format(latent_dim))\n",
    "weights_decoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:11:19.581521Z",
     "start_time": "2019-02-07T22:11:19.293528Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:81: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# Architecture of VAE\n",
    "dim_file =  os.path.join(os.path.dirname(os.getcwd()), \"metadata\", subsample_dataset, \"validation_tune_dim.pickle\")\n",
    "\n",
    "with open(dim_file, 'rb') as f:\n",
    "    num_samples, num_genes = pickle.load(f)\n",
    "\n",
    "original_dim = num_genes\n",
    "rnaseq_input = Input(shape=(original_dim, ))\n",
    "\n",
    "# Encoder\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "hidden_dense_linear = Dense(intermediate_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "hidden_dense_batchnorm = BatchNormalization()(hidden_dense_linear)\n",
    "hidden_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "# Note:\n",
    "# Normalize and relu filter at each layer adds non-linear component (relu is non-linear function)\n",
    "# If architecture is layer-layer-normalization-relu then the computation is still linear\n",
    "# Add additional layers in triplicate\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#z = Lambda(sampling,\n",
    "#           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "z = Lambda(sampling_maker(epsilon_std),\n",
    "               output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "# Decoder\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_model = Sequential()\n",
    "decoder_model.add(Dense(intermediate_dim, activation='relu', input_dim=latent_dim))\n",
    "decoder_model.add(Dense(original_dim, activation='sigmoid'))\n",
    "rnaseq_reconstruct = decoder_model(z)\n",
    "\n",
    "\n",
    "# Connections\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer(original_dim, z_log_var_encoded, z_mean_encoded, beta)([\n",
    "        rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:21:15.999784Z",
     "start_time": "2019-02-07T22:11:19.583111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "10/10 [==============================] - 6s 637ms/step - loss: 708.0984 - val_loss: 706.5732\n",
      "Epoch 2/100\n",
      "10/10 [==============================] - 6s 584ms/step - loss: 706.6116 - val_loss: 705.1453\n",
      "Epoch 3/100\n",
      "10/10 [==============================] - 6s 588ms/step - loss: 705.2828 - val_loss: 703.3218\n",
      "Epoch 4/100\n",
      "10/10 [==============================] - 6s 590ms/step - loss: 703.9293 - val_loss: 702.1812\n",
      "Epoch 5/100\n",
      "10/10 [==============================] - 6s 585ms/step - loss: 702.6951 - val_loss: 701.0727\n",
      "Epoch 6/100\n",
      "10/10 [==============================] - 6s 590ms/step - loss: 701.5259 - val_loss: 699.5498\n",
      "Epoch 7/100\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 700.4891 - val_loss: 698.8592\n",
      "Epoch 8/100\n",
      "10/10 [==============================] - 6s 591ms/step - loss: 699.1730 - val_loss: 697.8353\n",
      "Epoch 9/100\n",
      "10/10 [==============================] - 6s 611ms/step - loss: 698.0272 - val_loss: 696.8485\n",
      "Epoch 10/100\n",
      "10/10 [==============================] - 6s 604ms/step - loss: 696.9417 - val_loss: 696.0415\n",
      "Epoch 11/100\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 696.0922 - val_loss: 695.0580\n",
      "Epoch 12/100\n",
      "10/10 [==============================] - 6s 577ms/step - loss: 695.1367 - val_loss: 693.9428\n",
      "Epoch 13/100\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 694.2454 - val_loss: 693.2152\n",
      "Epoch 14/100\n",
      "10/10 [==============================] - 6s 592ms/step - loss: 693.1971 - val_loss: 692.5445\n",
      "Epoch 15/100\n",
      "10/10 [==============================] - 6s 569ms/step - loss: 692.2215 - val_loss: 691.6667\n",
      "Epoch 16/100\n",
      "10/10 [==============================] - 6s 571ms/step - loss: 691.4298 - val_loss: 690.9290\n",
      "Epoch 17/100\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 690.4860 - val_loss: 690.4541\n",
      "Epoch 18/100\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 689.7418 - val_loss: 689.4465\n",
      "Epoch 19/100\n",
      "10/10 [==============================] - 6s 562ms/step - loss: 688.9056 - val_loss: 688.9678\n",
      "Epoch 20/100\n",
      "10/10 [==============================] - 6s 561ms/step - loss: 688.1835 - val_loss: 688.1697\n",
      "Epoch 21/100\n",
      "10/10 [==============================] - 6s 561ms/step - loss: 687.3904 - val_loss: 687.4680\n",
      "Epoch 22/100\n",
      "10/10 [==============================] - 6s 563ms/step - loss: 686.6562 - val_loss: 687.0128\n",
      "Epoch 23/100\n",
      "10/10 [==============================] - 6s 563ms/step - loss: 685.9708 - val_loss: 686.2328\n",
      "Epoch 24/100\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 685.2948 - val_loss: 685.7292\n",
      "Epoch 25/100\n",
      "10/10 [==============================] - 6s 573ms/step - loss: 684.5289 - val_loss: 685.3256\n",
      "Epoch 26/100\n",
      "10/10 [==============================] - 6s 572ms/step - loss: 683.7939 - val_loss: 684.5652\n",
      "Epoch 27/100\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 683.3818 - val_loss: 684.2766\n",
      "Epoch 28/100\n",
      "10/10 [==============================] - 6s 567ms/step - loss: 682.7172 - val_loss: 683.2700\n",
      "Epoch 29/100\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 682.0925 - val_loss: 683.1152\n",
      "Epoch 30/100\n",
      "10/10 [==============================] - 6s 580ms/step - loss: 681.4896 - val_loss: 682.3574\n",
      "Epoch 31/100\n",
      "10/10 [==============================] - 6s 566ms/step - loss: 680.8554 - val_loss: 681.8930\n",
      "Epoch 32/100\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 680.3076 - val_loss: 681.3488\n",
      "Epoch 33/100\n",
      "10/10 [==============================] - 6s 599ms/step - loss: 679.7382 - val_loss: 680.9139\n",
      "Epoch 34/100\n",
      "10/10 [==============================] - 6s 610ms/step - loss: 679.0621 - val_loss: 680.3479\n",
      "Epoch 35/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 678.7042 - val_loss: 679.8489\n",
      "Epoch 36/100\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 678.0871 - val_loss: 679.3137\n",
      "Epoch 37/100\n",
      "10/10 [==============================] - 6s 596ms/step - loss: 677.6128 - val_loss: 678.9344\n",
      "Epoch 38/100\n",
      "10/10 [==============================] - 6s 607ms/step - loss: 677.1273 - val_loss: 678.5788\n",
      "Epoch 39/100\n",
      "10/10 [==============================] - 6s 610ms/step - loss: 676.5916 - val_loss: 677.9229\n",
      "Epoch 40/100\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 676.1286 - val_loss: 677.6310\n",
      "Epoch 41/100\n",
      "10/10 [==============================] - 6s 601ms/step - loss: 675.6450 - val_loss: 677.0738\n",
      "Epoch 42/100\n",
      "10/10 [==============================] - 6s 596ms/step - loss: 675.2375 - val_loss: 676.6616\n",
      "Epoch 43/100\n",
      "10/10 [==============================] - 6s 606ms/step - loss: 674.6919 - val_loss: 676.1848\n",
      "Epoch 44/100\n",
      "10/10 [==============================] - 6s 597ms/step - loss: 674.2692 - val_loss: 675.8023\n",
      "Epoch 45/100\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 673.7768 - val_loss: 675.4229\n",
      "Epoch 46/100\n",
      "10/10 [==============================] - 6s 604ms/step - loss: 673.3696 - val_loss: 675.1410\n",
      "Epoch 47/100\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 672.8764 - val_loss: 674.5615\n",
      "Epoch 48/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 672.5583 - val_loss: 674.2477\n",
      "Epoch 49/100\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 672.0791 - val_loss: 673.6696\n",
      "Epoch 50/100\n",
      "10/10 [==============================] - 6s 606ms/step - loss: 671.7948 - val_loss: 673.4171\n",
      "Epoch 51/100\n",
      "10/10 [==============================] - 6s 609ms/step - loss: 671.3889 - val_loss: 672.9740\n",
      "Epoch 52/100\n",
      "10/10 [==============================] - 6s 604ms/step - loss: 670.9818 - val_loss: 672.5909\n",
      "Epoch 53/100\n",
      "10/10 [==============================] - 6s 594ms/step - loss: 670.5666 - val_loss: 672.2929\n",
      "Epoch 54/100\n",
      "10/10 [==============================] - 6s 606ms/step - loss: 670.1136 - val_loss: 671.7495\n",
      "Epoch 55/100\n",
      "10/10 [==============================] - 6s 604ms/step - loss: 669.8373 - val_loss: 671.2346\n",
      "Epoch 56/100\n",
      "10/10 [==============================] - 6s 595ms/step - loss: 669.5143 - val_loss: 671.0011\n",
      "Epoch 57/100\n",
      "10/10 [==============================] - 6s 599ms/step - loss: 669.1411 - val_loss: 670.4879\n",
      "Epoch 58/100\n",
      "10/10 [==============================] - 6s 596ms/step - loss: 668.7305 - val_loss: 670.1898\n",
      "Epoch 59/100\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 668.3887 - val_loss: 670.0867\n",
      "Epoch 60/100\n",
      "10/10 [==============================] - 6s 608ms/step - loss: 668.0291 - val_loss: 669.6637\n",
      "Epoch 61/100\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 667.7252 - val_loss: 669.2225\n",
      "Epoch 62/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 667.3828 - val_loss: 668.9853\n",
      "Epoch 63/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 667.1560 - val_loss: 668.5465\n",
      "Epoch 64/100\n",
      "10/10 [==============================] - 6s 608ms/step - loss: 666.7862 - val_loss: 668.2315\n",
      "Epoch 65/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 666.4427 - val_loss: 668.0645\n",
      "Epoch 66/100\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 666.1453 - val_loss: 667.5356\n",
      "Epoch 67/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 665.8561 - val_loss: 667.3484\n",
      "Epoch 68/100\n",
      "10/10 [==============================] - 6s 599ms/step - loss: 665.4744 - val_loss: 666.7697\n",
      "Epoch 69/100\n",
      "10/10 [==============================] - 6s 607ms/step - loss: 665.2290 - val_loss: 666.5979\n",
      "Epoch 70/100\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 664.9759 - val_loss: 666.2038\n",
      "Epoch 71/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 664.6398 - val_loss: 665.9707\n",
      "Epoch 72/100\n",
      "10/10 [==============================] - 6s 595ms/step - loss: 664.4177 - val_loss: 665.7054\n",
      "Epoch 73/100\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 664.0797 - val_loss: 665.1931\n",
      "Epoch 74/100\n",
      "10/10 [==============================] - 6s 603ms/step - loss: 663.8749 - val_loss: 664.9599\n",
      "Epoch 75/100\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 663.5607 - val_loss: 664.7031\n",
      "Epoch 76/100\n",
      "10/10 [==============================] - 6s 597ms/step - loss: 663.3287 - val_loss: 664.3858\n",
      "Epoch 77/100\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 663.0721 - val_loss: 664.1153\n",
      "Epoch 78/100\n",
      "10/10 [==============================] - 6s 605ms/step - loss: 662.7889 - val_loss: 663.8579\n",
      "Epoch 79/100\n",
      "10/10 [==============================] - 6s 601ms/step - loss: 662.5182 - val_loss: 663.5454\n",
      "Epoch 80/100\n",
      "10/10 [==============================] - 6s 624ms/step - loss: 662.2833 - val_loss: 663.3000\n",
      "Epoch 81/100\n",
      "10/10 [==============================] - 6s 594ms/step - loss: 662.0523 - val_loss: 662.9215\n",
      "Epoch 82/100\n",
      "10/10 [==============================] - 6s 565ms/step - loss: 661.8202 - val_loss: 662.7730\n",
      "Epoch 83/100\n",
      "10/10 [==============================] - 6s 583ms/step - loss: 661.5430 - val_loss: 662.3495\n",
      "Epoch 84/100\n",
      "10/10 [==============================] - 6s 615ms/step - loss: 661.3492 - val_loss: 662.1108\n",
      "Epoch 85/100\n",
      "10/10 [==============================] - 6s 630ms/step - loss: 661.1465 - val_loss: 661.7325\n",
      "Epoch 86/100\n",
      "10/10 [==============================] - 6s 585ms/step - loss: 660.8247 - val_loss: 661.6184\n",
      "Epoch 87/100\n",
      "10/10 [==============================] - 6s 593ms/step - loss: 660.6993 - val_loss: 661.2647\n",
      "Epoch 88/100\n",
      "10/10 [==============================] - 6s 618ms/step - loss: 660.4862 - val_loss: 660.9487\n",
      "Epoch 89/100\n",
      "10/10 [==============================] - 6s 593ms/step - loss: 660.2330 - val_loss: 660.8836\n",
      "Epoch 90/100\n",
      "10/10 [==============================] - 6s 631ms/step - loss: 660.0034 - val_loss: 660.7036\n",
      "Epoch 91/100\n",
      "10/10 [==============================] - 6s 637ms/step - loss: 659.7857 - val_loss: 660.5955\n",
      "Epoch 92/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 659.6035 - val_loss: 660.1686\n",
      "Epoch 93/100\n",
      "10/10 [==============================] - 6s 607ms/step - loss: 659.3684 - val_loss: 660.0462\n",
      "Epoch 94/100\n",
      "10/10 [==============================] - 6s 606ms/step - loss: 659.1991 - val_loss: 659.8172\n",
      "Epoch 95/100\n",
      "10/10 [==============================] - 6s 600ms/step - loss: 658.9976 - val_loss: 659.4964\n",
      "Epoch 96/100\n",
      "10/10 [==============================] - 6s 598ms/step - loss: 658.8086 - val_loss: 659.3789\n",
      "Epoch 97/100\n",
      "10/10 [==============================] - 6s 606ms/step - loss: 658.6015 - val_loss: 659.0737\n",
      "Epoch 98/100\n",
      "10/10 [==============================] - 6s 602ms/step - loss: 658.4449 - val_loss: 658.8021\n",
      "Epoch 99/100\n",
      "10/10 [==============================] - 6s 604ms/step - loss: 658.2305 - val_loss: 658.6208\n",
      "Epoch 100/100\n",
      "10/10 [==============================] - 6s 594ms/step - loss: 658.1010 - val_loss: 658.4832\n",
      "CPU times: user 15min 12s, sys: 15.9 s, total: 15min 27s\n",
      "Wall time: 9min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit_generator(generator=training_generator,\n",
    "                         validation_data=validation_generator,\n",
    "                         shuffle=True,\n",
    "                         epochs=epochs,\n",
    "                         callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:21:16.020159Z",
     "start_time": "2019-02-07T22:21:16.001496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trained model\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:21:16.277606Z",
     "start_time": "2019-02-07T22:21:16.021707Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb205d11400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "#loss_df = pd.DataFrame(hist.history['loss'])\n",
    "#ax = loss_df.plot()\n",
    "ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-07T22:21:16.518093Z",
     "start_time": "2019-02-07T22:21:16.404685Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "#history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_model(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "decoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pa]",
   "language": "python",
   "name": "conda-env-Pa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
