{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T18:31:05.124719Z",
     "start_time": "2019-01-14T18:30:59.438857Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# -----------------------------------------------------------------------------------------------------------------------\n",
    "# By Alexandra Lee\n",
    "# (created December 2018) \n",
    "#\n",
    "# Encode Pseudomonas gene expression data into low dimensional latent space using \n",
    "# Tybalt with 2-hidden layers\n",
    "# \n",
    "# Note: Need to use python 3 to support '*' syntax change\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from functions.my_classes import DataGenerator\n",
    "from functions.helper_ae import sampling_maker, CustomVariationalLayer, WarmUpCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T18:31:05.166490Z",
     "start_time": "2019-01-14T18:31:05.130620Z"
    }
   },
   "outputs": [],
   "source": [
    "# To ensure reproducibility using Keras during development\n",
    "# https://keras.io/getting-started/faq/#how-can-i-obtain-reproducible-results-using-keras-during-development\n",
    "import numpy as np\n",
    "import random as rn\n",
    "\n",
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "# See these references for further details:\n",
    "# https://docs.python.org/3.4/using/cmdline.html#envvar-PYTHONHASHSEED\n",
    "# https://github.com/keras-team/keras/issues/2280#issuecomment-306959926\n",
    "randomState = 123\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(12345)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "# For further details, see: https://stackoverflow.com/questions/42022950/which-seeds-have-to-be-set-where-to-realize-100-reproducibility-of-training-res\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(1234)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)\n",
    "\n",
    "import keras\n",
    "from keras.layers import Input, Dense, Lambda, Layer, Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model, Sequential\n",
    "from keras import metrics, optimizers\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T18:31:05.246887Z",
     "start_time": "2019-01-14T18:31:05.168079Z"
    }
   },
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "# Initialize hyper parameters\n",
    "#\n",
    "# learning rate: \n",
    "# batch size: Total number of training examples present in a single batch\n",
    "#             Iterations is the number of batches needed to complete one epoch\n",
    "# epochs: One Epoch is when an ENTIRE dataset is passed forward and backward through the neural network only ONCE\n",
    "# kappa: warmup\n",
    "# original dim: dimensions of the raw data\n",
    "# latent dim: dimensiosn of the latent space (fixed by the user)\n",
    "#   Note: intrinsic latent space dimension unknown\n",
    "# epsilon std: \n",
    "# beta: Threshold value for ReLU?\n",
    "# --------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "learning_rate = 0.1#0.001\n",
    "epochs = 100\n",
    "kappa = 0.01\n",
    "\n",
    "intermediate_dim = 100\n",
    "latent_dim = 10\n",
    "epsilon_std = 1.0\n",
    "beta = K.variable(0)\n",
    "\n",
    "chunk_size = 100\n",
    "num_samples_train = 1319122\n",
    "num_samples_val = 1319\n",
    "\n",
    "\n",
    "train_file =  \"/home/alexandra/Documents/Data/LINCS/train_model_input.txt.xz\"\n",
    "validation_file = \"/home/alexandra/Documents/Data/LINCS/validation_model_input.txt.xz\"\n",
    "\n",
    "stat_file =  os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}latent_stats.tsv\".format(latent_dim))\n",
    "hist_plot_file = os.path.join(os.path.dirname(os.getcwd()), \"stats\", \"tybalt_2layer_{}latent_hist.png\".format(latent_dim))\n",
    "\n",
    "encoded_file = os.path.join(os.path.dirname(os.getcwd()), \"encoded\", \"train_input_2layer_{}latent_encoded.txt\".format(latent_dim))\n",
    "\n",
    "model_encoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_encoder_model.h5\".format(latent_dim))\n",
    "weights_encoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_encoder_weights.h5\".format(latent_dim))\n",
    "model_decoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_decoder_model.h5\".format(latent_dim))\n",
    "weights_decoder_file = os.path.join(os.path.dirname(os.getcwd()), \"models\", \"tybalt_2layer_{}latent_decoder_weights.h5\".format(latent_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T18:31:05.289950Z",
     "start_time": "2019-01-14T18:31:05.248443Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(train_file, chunk_size, num_samples_train)\n",
    "validation_generator = DataGenerator(validation_file, chunk_size, num_samples_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-14T18:31:05.565559Z",
     "start_time": "2019-01-14T18:31:05.292342Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/envs/Pa/lib/python3.5/site-packages/ipykernel/__main__.py:81: UserWarning: Output \"custom_variational_layer_1\" missing from loss dictionary. We assume this was done on purpose, and we will not be expecting any data to be passed to \"custom_variational_layer_1\" during training.\n"
     ]
    }
   ],
   "source": [
    "# Architecture of VAE\n",
    "dim_file =  os.path.join(os.path.dirname(os.getcwd()), \"metadata\", \"validation_dim.pickle\")\n",
    "\n",
    "with open(dim_file, 'rb') as f:\n",
    "    num_samples, num_genes = pickle.load(f)\n",
    "\n",
    "original_dim = num_genes\n",
    "rnaseq_input = Input(shape=(original_dim, ))\n",
    "\n",
    "# Encoder\n",
    "\n",
    "# Input layer is compressed into a mean and log variance vector of size\n",
    "# `latent_dim`. Each layer is initialized with glorot uniform weights and each\n",
    "# step (dense connections, batch norm,and relu activation) are funneled\n",
    "# separately\n",
    "# Each vector of length `latent_dim` are connected to the rnaseq input tensor\n",
    "\n",
    "# \"z_mean_dense_linear\" is the encoded representation of the input\n",
    "#    Take as input arrays of shape (*, original dim) and output arrays of shape (*, latent dim)\n",
    "#    Combine input from previous layer using linear summ\n",
    "# Normalize the activations (combined weighted nodes of the previous layer)\n",
    "#   Transformation that maintains the mean activation close to 0 and the activation standard deviation close to 1.\n",
    "# Apply ReLU activation function to combine weighted nodes from previous layer\n",
    "#   relu = threshold cutoff (cutoff value will be learned)\n",
    "#   ReLU function filters noise\n",
    "\n",
    "# X is encoded using Q(z|X) to yield mu(X), sigma(X) that describes latent space distribution\n",
    "hidden_dense_linear = Dense(intermediate_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "hidden_dense_batchnorm = BatchNormalization()(hidden_dense_linear)\n",
    "hidden_encoded = Activation('relu')(hidden_dense_batchnorm)\n",
    "\n",
    "# Note:\n",
    "# Normalize and relu filter at each layer adds non-linear component (relu is non-linear function)\n",
    "# If architecture is layer-layer-normalization-relu then the computation is still linear\n",
    "# Add additional layers in triplicate\n",
    "z_mean_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(hidden_encoded)\n",
    "z_mean_dense_batchnorm = BatchNormalization()(z_mean_dense_linear)\n",
    "z_mean_encoded = Activation('relu')(z_mean_dense_batchnorm)\n",
    "\n",
    "z_log_var_dense_linear = Dense(latent_dim, kernel_initializer='glorot_uniform')(rnaseq_input)\n",
    "z_log_var_dense_batchnorm = BatchNormalization()(z_log_var_dense_linear)\n",
    "z_log_var_encoded = Activation('relu')(z_log_var_dense_batchnorm)\n",
    "\n",
    "# Customized layer\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#\n",
    "# sampling():\n",
    "# randomly sample similar points z from the latent normal distribution that is assumed to generate the data,\n",
    "# via z = z_mean + exp(z_log_sigma) * epsilon, where epsilon is a random normal tensor\n",
    "# z ~ Q(z|X)\n",
    "# Note: there is a trick to reparameterize to standard normal distribution so that the space is differentiable and \n",
    "# therefore gradient descent can be used\n",
    "#\n",
    "# Returns the encoded and randomly sampled z vector\n",
    "# Takes two keras layers as input to the custom sampling function layer with a\n",
    "# latent_dim` output\n",
    "#z = Lambda(sampling,\n",
    "#           output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "z = Lambda(sampling_maker(epsilon_std),\n",
    "               output_shape=(latent_dim, ))([z_mean_encoded, z_log_var_encoded])\n",
    "\n",
    "# Decoder\n",
    "\n",
    "# The decoding layer is much simpler with a single layer glorot uniform\n",
    "# initialized and sigmoid activation\n",
    "# Reconstruct P(X|z)\n",
    "decoder_model = Sequential()\n",
    "decoder_model.add(Dense(intermediate_dim, activation='relu', input_dim=latent_dim))\n",
    "decoder_model.add(Dense(original_dim, activation='sigmoid'))\n",
    "rnaseq_reconstruct = decoder_model(z)\n",
    "\n",
    "\n",
    "# Connections\n",
    "# fully-connected network\n",
    "adam = optimizers.Adam(lr=learning_rate)\n",
    "vae_layer = CustomVariationalLayer(original_dim, z_log_var_encoded, z_mean_encoded, beta)([\n",
    "        rnaseq_input, rnaseq_reconstruct])\n",
    "vae = Model(rnaseq_input, vae_layer)\n",
    "vae.compile(optimizer=adam, loss=None, loss_weights=[beta])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T18:30:59.447Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "13191/13191 [==============================] - 894s 68ms/step - loss: 633.3437 - val_loss: 663.7040\n",
      "Epoch 2/100\n",
      "13191/13191 [==============================] - 878s 67ms/step - loss: 633.2263 - val_loss: 660.1549\n",
      "Epoch 3/100\n",
      "13191/13191 [==============================] - 900s 68ms/step - loss: 633.2178 - val_loss: 656.6269\n",
      "Epoch 4/100\n",
      "13191/13191 [==============================] - 897s 68ms/step - loss: 633.2203 - val_loss: 660.5884\n",
      "Epoch 5/100\n",
      "13191/13191 [==============================] - 896s 68ms/step - loss: 633.2247 - val_loss: 651.0439\n",
      "Epoch 6/100\n",
      "13191/13191 [==============================] - 896s 68ms/step - loss: 633.2188 - val_loss: 662.2886\n",
      "Epoch 7/100\n",
      "13191/13191 [==============================] - 904s 69ms/step - loss: 633.2188 - val_loss: 658.7274\n",
      "Epoch 8/100\n",
      "13191/13191 [==============================] - 894s 68ms/step - loss: 633.2155 - val_loss: 657.8793\n",
      "Epoch 9/100\n",
      "13191/13191 [==============================] - 897s 68ms/step - loss: 633.2174 - val_loss: 657.7915\n",
      "Epoch 10/100\n",
      "13191/13191 [==============================] - 923s 70ms/step - loss: 633.2118 - val_loss: 660.1925\n",
      "Epoch 11/100\n",
      "13191/13191 [==============================] - 911s 69ms/step - loss: 633.2117 - val_loss: 659.1277\n",
      "Epoch 12/100\n",
      "13191/13191 [==============================] - 895s 68ms/step - loss: 633.2114 - val_loss: 659.2351\n",
      "Epoch 13/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2115 - val_loss: 659.2484\n",
      "Epoch 14/100\n",
      "13191/13191 [==============================] - 886s 67ms/step - loss: 633.2263 - val_loss: 660.2641\n",
      "Epoch 15/100\n",
      "13191/13191 [==============================] - 901s 68ms/step - loss: 633.2201 - val_loss: 660.8556\n",
      "Epoch 16/100\n",
      "13191/13191 [==============================] - 888s 67ms/step - loss: 633.2190 - val_loss: 655.8077\n",
      "Epoch 17/100\n",
      "13191/13191 [==============================] - 902s 68ms/step - loss: 633.2184 - val_loss: 656.6414\n",
      "Epoch 18/100\n",
      "13191/13191 [==============================] - 903s 68ms/step - loss: 633.2128 - val_loss: 657.0011\n",
      "Epoch 19/100\n",
      "13191/13191 [==============================] - 903s 68ms/step - loss: 633.2106 - val_loss: 656.8133\n",
      "Epoch 20/100\n",
      "13191/13191 [==============================] - 903s 68ms/step - loss: 633.2105 - val_loss: 656.1599\n",
      "Epoch 21/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2136 - val_loss: 652.7415\n",
      "Epoch 22/100\n",
      "13191/13191 [==============================] - 904s 69ms/step - loss: 633.2182 - val_loss: 663.0333\n",
      "Epoch 23/100\n",
      "13191/13191 [==============================] - 904s 69ms/step - loss: 633.2302 - val_loss: 662.5718\n",
      "Epoch 24/100\n",
      "13191/13191 [==============================] - 904s 68ms/step - loss: 633.2236 - val_loss: 653.4513\n",
      "Epoch 25/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2194 - val_loss: 656.2287\n",
      "Epoch 26/100\n",
      "13191/13191 [==============================] - 903s 68ms/step - loss: 633.2207 - val_loss: 655.8016\n",
      "Epoch 27/100\n",
      "13191/13191 [==============================] - 902s 68ms/step - loss: 633.2255 - val_loss: 653.8149\n",
      "Epoch 28/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2269 - val_loss: 651.8869\n",
      "Epoch 29/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2272 - val_loss: 648.0518\n",
      "Epoch 30/100\n",
      "13191/13191 [==============================] - 904s 69ms/step - loss: 633.2186 - val_loss: 656.5106\n",
      "Epoch 31/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2168 - val_loss: 651.0346\n",
      "Epoch 32/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2164 - val_loss: 651.1144\n",
      "Epoch 33/100\n",
      "13191/13191 [==============================] - 903s 68ms/step - loss: 633.2163 - val_loss: 650.5757\n",
      "Epoch 34/100\n",
      "13191/13191 [==============================] - 904s 69ms/step - loss: 633.2171 - val_loss: 650.9745\n",
      "Epoch 35/100\n",
      "13191/13191 [==============================] - 903s 68ms/step - loss: 633.2220 - val_loss: 652.1660\n",
      "Epoch 36/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2197 - val_loss: 653.0968\n",
      "Epoch 37/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2177 - val_loss: 651.9706\n",
      "Epoch 38/100\n",
      "13191/13191 [==============================] - 904s 69ms/step - loss: 633.2177 - val_loss: 652.3749\n",
      "Epoch 39/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2174 - val_loss: 652.2522\n",
      "Epoch 40/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2174 - val_loss: 651.8582\n",
      "Epoch 41/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2188 - val_loss: 652.0045\n",
      "Epoch 42/100\n",
      "13191/13191 [==============================] - 904s 69ms/step - loss: 633.2273 - val_loss: 651.3753\n",
      "Epoch 43/100\n",
      "13191/13191 [==============================] - 903s 68ms/step - loss: 633.2166 - val_loss: 667.6342\n",
      "Epoch 44/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2123 - val_loss: 655.7934\n",
      "Epoch 45/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2104 - val_loss: 654.6082\n",
      "Epoch 46/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2104 - val_loss: 654.5863\n",
      "Epoch 47/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2108 - val_loss: 654.0741\n",
      "Epoch 48/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2103 - val_loss: 654.2452\n",
      "Epoch 49/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2084 - val_loss: 656.1543\n",
      "Epoch 50/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2105 - val_loss: 654.4438\n",
      "Epoch 51/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2102 - val_loss: 654.2582\n",
      "Epoch 52/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2102 - val_loss: 654.5668\n",
      "Epoch 53/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2223 - val_loss: 647.4093\n",
      "Epoch 54/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2215 - val_loss: 671.1300\n",
      "Epoch 55/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2186 - val_loss: 653.8005\n",
      "Epoch 56/100\n",
      "13191/13191 [==============================] - 908s 69ms/step - loss: 633.2161 - val_loss: 656.2774\n",
      "Epoch 57/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2204 - val_loss: 657.3546\n",
      "Epoch 58/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2214 - val_loss: 655.8902\n",
      "Epoch 59/100\n",
      "13191/13191 [==============================] - 908s 69ms/step - loss: 633.2292 - val_loss: 661.5455\n",
      "Epoch 60/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2349 - val_loss: 659.0909\n",
      "Epoch 61/100\n",
      "13191/13191 [==============================] - 908s 69ms/step - loss: 633.2254 - val_loss: 663.5831\n",
      "Epoch 62/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2330 - val_loss: 660.2483\n",
      "Epoch 63/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2311 - val_loss: 657.7795\n",
      "Epoch 64/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2210 - val_loss: 664.1739\n",
      "Epoch 65/100\n",
      "13191/13191 [==============================] - 908s 69ms/step - loss: 633.2170 - val_loss: 656.6044\n",
      "Epoch 66/100\n",
      "13191/13191 [==============================] - 906s 69ms/step - loss: 633.2207 - val_loss: 652.2179\n",
      "Epoch 67/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2266 - val_loss: 656.4517\n",
      "Epoch 68/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2269 - val_loss: 659.1053\n",
      "Epoch 69/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2274 - val_loss: 656.1439\n",
      "Epoch 70/100\n",
      "13191/13191 [==============================] - 908s 69ms/step - loss: 633.2271 - val_loss: 648.7527\n",
      "Epoch 71/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2144 - val_loss: 667.4113\n",
      "Epoch 72/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2418 - val_loss: 662.3115\n",
      "Epoch 73/100\n",
      "13191/13191 [==============================] - 902s 68ms/step - loss: 633.2328 - val_loss: 657.0747\n",
      "Epoch 74/100\n",
      "13191/13191 [==============================] - 902s 68ms/step - loss: 633.2218 - val_loss: 658.3124\n",
      "Epoch 75/100\n",
      "13191/13191 [==============================] - 902s 68ms/step - loss: 633.2207 - val_loss: 654.3257\n",
      "Epoch 76/100\n",
      "13191/13191 [==============================] - 902s 68ms/step - loss: 633.2256 - val_loss: 650.3028\n",
      "Epoch 77/100\n",
      "13191/13191 [==============================] - 902s 68ms/step - loss: 633.2291 - val_loss: 653.1216\n",
      "Epoch 78/100\n",
      "13191/13191 [==============================] - 905s 69ms/step - loss: 633.2188 - val_loss: 665.2675\n",
      "Epoch 79/100\n",
      "13191/13191 [==============================] - 907s 69ms/step - loss: 633.2150 - val_loss: 654.9724\n",
      "Epoch 80/100\n",
      "13191/13191 [==============================] - 911s 69ms/step - loss: 633.2160 - val_loss: 652.6408\n",
      "Epoch 81/100\n",
      " 5434/13191 [===========>..................] - ETA: 8:58 - loss: 633.5381- ETA: 9:04 - loss: 633.798 - ETA: 9:04 - lo - ETA: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Training\n",
    "# hist: record of the training loss at each epoch\n",
    "hist = vae.fit_generator(generator=training_generator,\n",
    "                         validation_data=validation_generator,\n",
    "                         shuffle=True,\n",
    "                         epochs=epochs,\n",
    "                         callbacks=[WarmUpCallback(beta, kappa)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T18:30:59.448Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trained model\n",
    "encoder = Model(rnaseq_input, z_mean_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T18:30:59.450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "loss_df = pd.DataFrame(hist.history['loss'])\n",
    "ax = loss_df.plot()\n",
    "#ax = history_df.plot()\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('VAE Loss')\n",
    "fig = ax.get_figure()\n",
    "fig.savefig(hist_plot_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T18:30:59.451Z"
    }
   },
   "outputs": [],
   "source": [
    "loss = hist.history['loss']\n",
    "val_loss = hist.history['val_loss']\n",
    "epochs = range(epochs)\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'o', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-14T18:30:59.453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Output\n",
    "\n",
    "# Save training performance\n",
    "history_df = pd.DataFrame(hist.history)\n",
    "history_df = history_df.assign(learning_rate=learning_rate)\n",
    "#history_df = history_df.assign(batch_size=batch_size)\n",
    "history_df = history_df.assign(epochs=epochs)\n",
    "history_df = history_df.assign(kappa=kappa)\n",
    "history_df.to_csv(stat_file, sep='\\t')\n",
    "\n",
    "# Save models\n",
    "# (source) https://machinelearningmastery.com/save-load-keras-deep-learning-models/\n",
    "\n",
    "# Save encoder model\n",
    "encoder.save(model_encoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "encoder.save_weights(weights_encoder_file)\n",
    "\n",
    "# Save decoder model\n",
    "# (source) https://github.com/greenelab/tybalt/blob/master/scripts/nbconverted/tybalt_vae.py\n",
    "decoder_input = Input(shape=(latent_dim, ))  # can generate from any sampled z vector\n",
    "_x_decoded_mean = decoder_model(decoder_input)\n",
    "decoder = Model(decoder_input, _x_decoded_mean)\n",
    "\n",
    "decoder.save(model_decoder_file)\n",
    "    \n",
    "# serialize weights to HDF5\n",
    "decoder.save_weights(weights_decoder_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pa]",
   "language": "python",
   "name": "conda-env-Pa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
